{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Podcast project\\venv\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from groq import Groq\n",
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from pydub import AudioSegment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "client = Groq(api_key=API_KEY)\n",
    "model = 'whisper-large-v3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_text(filepath):\n",
    "    with open (filepath ,'rb') as file:\n",
    "        translation = client.audio.translations.create(\n",
    "            file = (filepath , file.read()),\n",
    "            model=\"whisper-large-v3\"\n",
    "        )\n",
    "        \n",
    "    return translation.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"Groq's AI Chip Breaks Speed Records.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Welcome back, you're with Connect The World. I'm Becky Anderson. We're at the World Government Summit in Dubai and one thing I've noticed here is that whenever a discussion about artificial intelligence takes place, the rooms here, the huge halls get packed. That is because some of the leading minds behind the technological revolution have been gathered here at the conference over the past couple of days and in AI's race to the top my next guest is sprinting at speeds never seen before Jonathan Ross is the brain behind GROK the world's first language processing unit now before I lose you in the technological jargon of AI let me put it this way what Ross created is a chip that can run programs like Meta's Llama 2 model for example faster than anything else in the world. 10 to 100 times faster in fact and he's here with me now to explain how that is possible. Before I ask you that, Grok, why Grok? Thank you Becky. It's Grok and we spell it with a Q and it's because it comes from a science fiction novel and it means to understand something deeply and with empathy. Of course it Tell us about your chip and what makes Brock Chip LPU different from other AI chips and accelerators. I have to tell our viewers that the NVIDIA CEO was here, of course, this week at the beginning of the week. So we've had all the greatest minds in here. What's your story? Well, asking me how the chip works before I show you what it does is a bit like asking how a magic trick works before showing you the magic trick. You're going to get bored, but I'll give it a shot. Cool. So most chips they don have enough memory inside of them Sort of like if you were building cars and you use a giant factory and you need about a million square feet of assembly line space Well if you don have a building that large enough to fit that, then you need to set up part of the assembly line, tear it down over and over again. Right. And that's slow and it takes a lot of time. And that's what happens with the GPU. You \n"
     ]
    }
   ],
   "source": [
    "translation_text = audio_to_text(filepath)\n",
    "print(translation_text[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the transcript, Jonathan Ross, the brain behind GROK, emphasizes the importance of fast language models. He states:\n",
      "\n",
      "\"Imagine if I spoke, if I spoke that slowly, you'd just drift off, you'd go away. Most certainly. So the statistic is if you improve the speed by a hundred milliseconds on a website on desktop, you will get about an eight percent increase in user engagement. On mobile it's 34 percent. People have no patience on mobile...\"\n",
      "\n",
      "He also mentions:\n",
      "\n",
      "\"The reason you care about the speed is it's about engagement. Imagine if I spoke, if I spoke that slowly, you'd just drift off, you'd go away. Most certainly. So the statistic is if you improve the speed by a hundred milliseconds on a website on desktop you will get about an eight percent increase in user engagement...\"\n",
      "\n",
      "This highlights the crucial role that speed plays in language models. The faster the model can process and respond to user input, the more engaging and interactive the experience will be for users.\n",
      "\n",
      "In 2024, as predicted by Ross, AI is expected to become more natural and widespread, making it even more important for language models to be fast and responsive. He states:\n",
      "\n",
      "\"This technology is getting better and better every single day. Right now, it's at a point where for most people when they're accessing it, it's unnatural, it's slow. This is going to make it more natural. But the model that you were interacting with, while very good, is not quite as good as OpenAI's model. That natural experience, though, changes it incredibly.\"\n",
      "\n",
      "By improving the speed of language models, developers can increase user engagement, enhance the overall user experience, and make AI more accessible and natural for everyone.\n",
      "\n",
      "In the context of language processing, speed is crucial because it can significantly impact our ability to interact with language-based systems. When language models respond quickly, users can have a more seamless and natural conversation-like experience, which can lead to:\n",
      "\n",
      "1. Increased user engagement and satisfaction\n",
      "2. Improved accuracy and effectiveness in tasks\n",
      "3. Enhanced overall user experience\n",
      "\n",
      "In summary, fast language models are important because they enable faster and more responsive interactions, leading to increased user engagement, satisfaction, and overall experience. As AI becomes more widespread and natural in 2024, the demand for faster and more efficient language models will continue to grow.\n"
     ]
    }
   ],
   "source": [
    "# 3. Transcript chat completion \n",
    "def transcript_chat_completion(client, transcript, user_question):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\":'''use this transcript or transcripts to answer any user questions,citing specific quotes:\n",
    "                {transcript}\n",
    "                '''.format(transcript=transcript)\n",
    "            },\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":user_question,\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "        \n",
    "    )\n",
    "    print(chat_completion.choices[0].message.content)\n",
    "    \n",
    "    \n",
    "    \n",
    "user_question  = \"explain the importance of fast language models\"\n",
    "transcript_chat_completion(client,translation_text,user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
